\documentclass[11pt]{article}

\usepackage{graphicx}
%\usepackage{algorithmic}
%\usepackage{algorithm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[mathscr]{euscript}
\usepackage{mathtools}
\usepackage{amsmath}

\begin{document}

\pagenumbering{arabic}

\begin{center}
{\LARGE{\textbf{Uncertainty in Modeling and Reasoning About Beliefs}}} \\
\Large\textsc{Ph.D. Comprehensive Exam} \\[1em]
\large\textnormal{Mohammad Shayganfar - mshayganfar@wpi.edu} \\
\large\textnormal{May, 26 2015}
\end{center}

\section{Introduction to Uncertainty in AI}

In many practical problem-solving applications, the available knowledge is
incomplete or inexact. In such cases, the available knowledge is inadequate to
support logical reasonings. Humans use some techniques such as generalization
and approximation when they are confronted with uncertain information about 
decisions they should make. Although these techniques are subject to error,
humans still use them. Most of the time the sources of uncertainty, e,g.,
missing or noisy data, or incomplete knowledge of a causality, are unpredictable
or even unavoidable. Therefore, when we design intelligent machines, they
should be able to operate in uncertain and sometimes ambiguous environments, and
yet provide the most acceptable results for us according to the given
conditions. The reasoning methods under uncertainty allow AI systems to use
uncertain knowledge and make decisions while minimizing risk
\cite{tanimoto:ai-lisp}.

There are some major theories of uncertainty in reasoning about beliefs in AI.
In this response, first, we provide three of the most prominent theories
including \textit{Bayesian Belief Networks} (probabilistic reasoning),
\textit{Dempster-Shafer} theory (evidential reasoning), and \textit{Fuzzy Logic}
(reasoning under ambiguity) -- see Sections \ref{sec:bbn}, \ref{sec:dst}, and
\ref{sec:flt} respectively. There are other approaches such as rule-based
systems which emerged from early work on practical and intuitive systems for
logical inference \cite{russell:ai-modern}. However, we do not review these
approaches due to their relatively low level of importance in the reviewed
literature and time. This document continues by providing the advantages and
disadvantages of the three major theories we mentioned above (see Section
\ref{sec:pros-cons}). Then, we briefly provide applications of the Bayesian
networks for robots and autonomous agents (see Section \ref{sec:applications}).
This document ends with our conclusion about the theories of reasoning about
beliefs under uncertainty (see Section \ref{sec:conclusion}).

\section{Theories of Uncertainty}

The major theories of uncertainty in AI each seek to address different aspects
of uncertainty. The underlying principles of each of these theories are outlined
in the following sections.

\subsection{Bayesian Belief Networks}
\label{sec:bbn}

A \textit{Bayesian Belief Network} \cite{pearl:probabilistic-reasoning} is a 
directed acyclic graph consisting of nodes and edges which provides a graphical
model for reasoning under uncertainty. Each node in the network represents a
random variable from the domain. The state of each node is called
\textit{belief}, which based on the prior evidence reflects the posterior
probability distribution of the other values associated with that node. Each
node also has an associated \textit{Conditional Probability Table} (CPT) which
represents the conditional probability of the variable given the value of its
parents in the graph. Each individual edge between two variables represents the
relation or conditional dependence between those two variables. Also, the
explicit directions represented by arrows as directional edges indicate the
notion of causality in the network (see Figure \ref{fig:bbn}). They are always
drawn from cause nodes to effect nodes, indicating dependencies between
variables \cite{das:decision-making-agents}. Assuming discrete variables, the
strength of the relationship between variables is quantified by conditional
probability distributions associated with each node.

Constructing a belief network can be divided into two different subtasks: a)
specifying the causal structure among the existing variables in the network, and
b) specifying the prior and conditional probabilities for these variables.

\subsubsection{Network's Structure}

The structure, or topology, of the network captures qualitative relationships
between variables (see Figure \ref{fig:bbn}). The first step in building the
Bayesian network's structure is to determine a) what are the nodes/variables to
represent in the structure, and b) what are their possible values? For instance,
nodes with discrete values can have boolean (to represent that a proposition is
true or false), ordered (e.g., enumeration), and integral values (e.g., height
of a person). Then, one should determine the existing causality between nodes,
i.e., to determine which node (parent) influences the other (child) and connect
them through directed edges \cite{korb:bayesian-ai}.

\begin{figure}[tbh]
  \center
  \includegraphics[width=0.6\textwidth]{figure/bbn.png}
  \caption{A Belief Network for a lung cancer problem
  \cite{korb:bayesian-ai}.}
  \label{fig:bbn}
\end{figure}

\subsubsection{Conditional Probability Table}

As we mentioned earlier, after specifying the structure of a Bayesian
Network, the next step is to quantify the relationships between connected nodes
by specifying the conditional probability distribution for each node. These
conditional probability distributions appear as \textit{Conditional Probability
Tables} (CPT) if we consider discrete variables in the structure. To calculate
values in CPTs, for each node, we need to think about all possible combinations
of values of parent nodes. Each row in a CPT will contain the value of a
conditional probability of a node for each case of the possible combination of
values for the parent node. Clearly, a node that has many parents or a node with
the parents taking a large number of values, can cause very large CPTs. The size
of the CPT is exponentially related to the number of parents. For instance, if
the nodes of a network are boolean, a variable with \textit{n} parents requires
a CPT with $2^{n+1}$ probabilities. The probabilities in a CPT are typically
acquired from experts on the subject, but they can also be learned automatically
using machine learning approaches. Figure \ref{fig:bbn} shows variables, their
relations, and associated CPTs for diagnosis of a lung cancer problem taken from
\cite{korb:bayesian-ai}.

\subsubsection{Markov Property}
\label{sec:markov-property}

In Bayesian networks, each variable is independent of its non-descendants given
its parent variables. Therefore, there are no direct dependencies in the system
being modeled other than those already explicitly shown via edges. Meaning,
there is no hidden connection between variables. This is called \textit{Markov
property} in a Bayesian network. If Bayesian networks do not adhere to Markov
property, there will be redundant edges that connect independent variables
together. Consequently, the network will not represent a minimal model.

\subsubsection{Joint Probability Distribution}

In many applications of probability, there are more than one random variable to
be measured over the same sample space (e.g., existence of multiple causes
for a lung cancer). A Bayesian network provides a complete description of the
domain. Once we identify random variables and their probabilistic relationships,
the values in a joint probability distribution can then be obtained from the
probabilities relating the random variables. Therefore, all the entries in the
full joint probability distribution can be calculated from the information in
the network. There is also a fundamental assumption that in the underlying
structure of the problem being modeled by a Bayesian network, not every single
node is connected to every other one \cite{korb:bayesian-ai}. Therefore, if
there is such a problem structure, then a Bayesian network can provide a compact
representation of a model for that problem. In the following formula
\textit{P($x_1, x_2, \ldots, x_n$)} is an abbreviation for the conjunction of
\textit{\textbf{n}} assignments to each variable. Hence, the following formula
gives the value of each variable:

\begin{center}
$P(x_1, x_2, \ldots, x_n) = \prod\limits_{i=1}^{n} P(x_i | parents (X_i))$\\	
\end{center}

\noindent where \textit{parents($X_i$)} denotes the specific values of the
variables in \textit{Parents($X_i$)}. As we see, given Markov property, the
product of only the appropriate elements (parent nodes) of the CPTs in the
network represents the value of each individual entry in the joint probability
distribution. The following provides an example based on the network provided in
Figure \ref{fig:bbn}.\\

\begin{footnotesize}
\noindent $P(X= pos \wedge D = true \wedge C = false \wedge P = high \wedge S =
true)$\\

\noindent $= P(X= pos | D = true , C = false , P = high , S = true)$

\noindent $\times P(D = true | C = false , P = high , S = true)$

\noindent $\times P(C = false | P = high , S = true) \times P(P = high| S =
true) \times P(S = true)$\\

\noindent $= P(X= pos | C = false) \times P(D = true | C = false) \times P(C =
false | P = high , S = true) \times P(P = high) \times P(S = true)$
\end{footnotesize}

\subsubsection{Reasoning in Bayesian Networks}

Reasoning in Bayesian networks is the process of updating beliefs in the face of
evidence. In other words, it is the process of efficiently deducing the belief
distribution over a particular subset of random variables given that we know the
states of some other variables in the network. Bayesian networks can be
conditioned upon any subset of their variables, supporting any direction of
reasoning. Figure \ref{fig:reasoning-types} shows four different types of
reasoning using the network shown in Figure \ref{fig:bbn}. These four types are
reasoning are \cite{korb:bayesian-ai}:\\

\textbf{Diagnostic reasoning} -- This is the reasoning from symptoms (effects)
to cause. For instance, a doctor updates her belief about a patient's cancer
when she checks the X-ray results.

\textbf{Predictive reasoning} -- This is the reasoning based on new information
about the causes to new beliefs about the corresponding effects. For instance,
if the patient tells his doctor the information about the polluted area he
lives in, the doctor's belief about the patient having cancer increases, even
without assessing the patient's symptoms.

\textbf{Intercausal reasoning} -- This is the reasoning about the mutual causes
of a common effect. For instance,  suppose that there are two different causes
for lung cancer, smoking and pollution (see Figure \ref{fig:bbn}). Initially
these two causes are independent of each other; i.e., the patient smoking or
not, does not change the probability of the patient being subject to pollution.
However, as soon as the patient is diagnosed with cancer, the probability of
smoking or living in a polluted area increases. Now, if the doctor discovers
that her patient is a smoker, then the probability of him living in polluted
area decreases. Therefore, the presence of one explanatory cause for the cancer
lowers the probability of the alternative cause, even though they both were
independent causes. In other words, the first explanatory cause \textit{explains
away} the alternative one.

\textbf{Combined reasoning} -- Sometimes the reasoning does not fit into one of
the explained types. Thus, any of these reasoning types can be combined to solve
a problem.

\begin{figure}[tbh]
  \center
  \includegraphics[width=0.65\textwidth]{figure/reasoning-types.png}
  \caption{Types of Reasoning \cite{korb:bayesian-ai}.}
  \label{fig:reasoning-types}
\end{figure}

\subsubsection{Conditional Independence}
\label{sec:conditional-independence}

Bayesian networks which satisfy the Markov property (see Section
\ref{sec:markov-property}) explicitly express conditional independencies in
probability distributions \cite{korb:bayesian-ai}. Therefore, since a Bayesian
Network is based on a joint probability distribution of a set of random
variables, knowledge about the conditional independence of these random
variables is important for understanding reasoning based on conditional
probabilities.

Two random variables \textit{A} and \textit{B} are \textit{conditionally
independent} given another variable \textit{C}, if $p(A,B|C) = p(A|C).p(B|C)$,
therefore:

\begin{center}
$p(A|B,C) = \frac{p(A,B|C)}{p(B|C)} = \frac{p(A|C).p(B|C)}{p(B|C)} = p(A|C)$
\end{center}

\noindent And similarly, $p(B|A,C) = p(B|C)$. Figure
\ref{fig:conditional-independence}(a) shows a \textit{causal chain} between
\textit{A} and \textit{B} and \textit{C}. For instance, being a smoker can cause
lung cancer which causes shortness of breath, in our example. This kind of
causal chains can cause a conditional independence which can be described as:
$P(C|A,B) = P(C|B)$. This means that if one already knows that \textit{C} has
occurred, knowing that \textit{A} occurred doesnâ€™t make a difference to one's
beliefs about \textit{C}. Figure \ref{fig:conditional-independence}(b) shows
that both variables \textit{A} and \textit{C} have a \textit{common cause}
called \textit{B}. For instance, based on our example, lung cancer is a common
cause for a positive x-ray and dyspnoea in the patient. This kind of common
causes can also cause a conditional independence which, again, can be described
as: $P(C|A,B) = P(C|B)$. This means that if one already knows about \textit{B},
then an additional information that \textit{A} provides, will not give more
information about the chances of \textit{C}. Figure
\ref{fig:conditional-independence}(c) shows that one variable has two causes.
\textit{Common effect} produces the opposite conditional independence to that of
common causes and causal chains. This means that parents are independent until
the common effect provides new information. This kind of common effects can
cause a conditional dependence which can be described as: $P(A|B,C) \neq
P(A|B)$. In other words, if one knows about \textit{B} (the effect), then finds
out that for example \textit{A} (one of two causes) is absent, this increases
the probability of \textit{C} (alternative cause).

\begin{figure}[tbh]
  \center
  \includegraphics[width=0.8\textwidth]{figure/conditional-independence.png}
  \caption{(a) causal chain, (b) common cause, and (c) common effect
  \cite{korb:bayesian-ai}.}
  \label{fig:conditional-independence}
\end{figure}

\subsubsection{d-Separation}

The concepts of conditional dependencies and independencies, discussed above,
can apply not only between pairs of nodes, but also between sets of nodes. In
general, it is possible to determine whether two sets of nodes \textit{X} and
\textit{Y} are independent, if there is a set of evidence nodes \textit{E},
given the Markov property. If the two sets of nodes \textit{X} and \textit{Y}
are \textit{d-separated} (directional-dependent separation) by an evidence set
of nodes \textit{E}, then (given the Markov property) the two sets of nodes
\textit{X} and \textit{Y} are conditionally independent given \textit{E}.
d-separation is a topological criterion for Bayesian networks
\cite{russell:ai-modern}. Figure \ref{fig:d-separation} shows how the evidence
set of nodes \textit{E} is blocking the two sets of nodes \textit{X} and
\textit{Y} in three different conditions. In a graph, a path is blocked given a
set of nodes \textit{E}, if there is a node \textit{Z} on the path for which at
least one of the three conditions discussed in Section
\ref{sec:conditional-independence} holds.

\begin{figure}[tbh]
  \center
  \includegraphics[width=0.55\textwidth]{figure/d-separation.png}
  \caption{Three types of situations in which the path from \textit{X} to
  \textit{Y} can be blocked, given evidence \textit{E}. In each case, \textit{X}
  and \textit{Y} are d-separated by \textit{E}
  \cite{korb:bayesian-ai}.}
  \label{fig:d-separation}
\end{figure}

Analogous to our example, based on this definition the pollution and smoking
variables are d-separated from x-ray and dyspnoea (blocking condition 1), x-ray
is d-separated from dyspnoea (blocking condition 2), and if cancer and x-ray or
dyspnoea are not observed, then smoking variable would have been d-separated
from pollution (blocking condition 3).

\subsection{Dempster-Shafer Theory}
\label{sec:dst}

In \cite{dempster:theory}, Dempster proposed a probabilistic framework based on
lower and upper bounds on probabilities. In \cite{shafer:evidence-theory},
Shafer developed a formalism for reasoning under uncertainty which uses some of
Dempster's mathematical expressions with a different interpretation. Based on
Shafer's formalism, each piece of evidence may support a subset containing
several hypotheses. This is a generalization of the pure probabilistic framework
in which every finding corresponds to a value of a variable (a single
hypothesis) \cite{diez:reasoning-uncertainty}. Therefore, Dempster-Shafer theory
is the generalization of the Bayesian theory of subjective probability to
combine accumulative evidence or to change prior opinions in the light of new
evidence \cite{das:decision-making-agents}. Dempster-Shafer theory is designed
to deal with the distinction between uncertainty and ignorance. Rather than
computing the probability of a proposition, it computes the probability that the
evidence supports the proposition \cite{russell:ai-modern}, and it does not
require the assumption that \textit{Belief}(A) + \textit{Belief}($\neg$A) = 1.
Dempster-Shafer theory deals with the possible values of an unknown variable,
just as the theory of probability does \cite{tanimoto:ai-lisp}.

There are three basic functions in the Dempster-Shafer theory that we need to
understand for modeling purposes, \textit{mass function, belief function}, and
\textit{plausibility function}. Let $\Theta=\{h_1,h_2, \ldots, h_n\}$ be a
finite set of hypotheses. This set of hypotheses is also called \textit{frame of
discernment}. The hypotheses represent all of the possible states of the system
considered. The set of all subsets of $\Theta$ is its \textit{power set}:
$2^\Theta$. A subset of these $2^\Theta$ sets may consist of a single hypothesis
or of a conjunction of several hypotheses (e.g., a snowy day and a dry day). The
pieces of evidence are events that occurred or may occur (e.g., high pressure
shown by a barometer, or low temperature). One piece of evidence can be related
to a single hypothesis or a set of hypotheses. However, it is not allowed to
have different pieces of evidence lead to the same hypothesis or set of
hypotheses. In fact, the relation between a piece of evidence and a hypothesis
corresponds to a cause-effect chain, i.e., a piece of evidence implies a
hypothesis or a set of hypotheses \cite{kay:dst-reliability}. Moreover, it is
required that all hypotheses are unique, not overlapping and mutually exclusive.

\subsubsection{Mass Function}

A \textit{Basic Probability Assignment} (BPA) or \textit{mass function}
is a function $m:2^\Theta\rightarrow[0,1]$ such that:\\

\textit{m}($\emptyset$) = 0, and $\sum\limits_{x\in2^\Theta}m(x) =1$.\\

The value 0 indicates no belief and the value 1 indicates total belief, and
any value between these two indicate partial belief. As you see the mass
function uses the notion of $2^\Theta$ to be able to use all possible subsets of
the \textit{frame of discernment} $\Theta$. All of the assigned probabilities
sum to unity. There is no belief in an empty set. Any subset x of the frame of
discernment $\Theta$ for which m(x) is non-zero is called a \textit{focal
element} and represents the exact belief in the proposition depicted by x. Thus,
any subset is proposition and vice versa. Other elements in Dempster-Shafer
theory are defined by mass function. 

\subsubsection{Belief Function}

Now, we can define another important notion in Dempster-Shafer theory, the
\textit{belief function} (sometimes called a \textit{support function}). It is
the measure of total belief committed to $A \subseteq \Theta$ that can be
obtained by simply adding up the mass of all the subsets of \textit{A}. In other
words, given the frame of discernment $\Theta$ and $A \subseteq \Theta$, the
belief in \textit{A}, denoted \textit{Belief}(\textit{A}), is a number in the
interval [0, 1]. Belief in a set of elements, say \textit{A}, of a frame
$\Theta$, represents the total belief that one has based on the evidence
obtained. Unlike probability theory, \textit{Belief}(\textit{A}) = 0 represents
lack of evidence about \textit{A}, while \textit{p}(A) = 0 represents the
impossibility of \textit{A}. However, \textit{Belief}(\textit{A}) = 1 represents
certainty, that is \textit{A} is certain to occur, similar to \textit{p}(A) = 1,
which also represents the certainty that \textit{A} is true. A belief function
defined on a space $\Theta$ must satisfy the following three properties:

\begin{itemize}
	\item[]\textit{Belief}($\emptyset$) = 0
	\item[]\textit{Belief}($\Theta$) = 1
	\item[]\small\textit{Belief}($A_1 \cup \ldots A_n$)
	$\geq \sum\limits_i\textit{Belief}$($A_i$) - $\sum\limits_{i \textless
	j}\textit{Belief}$($A_i \cap A_j$) + \ldots +\\
	$(-1)^{n+1}\textit{Belief}$($A_i \cap \ldots \cap A_n$)
\end{itemize}

\noindent A belief function is a function $Belief:2^\Theta\rightarrow[0,1]$ and
is defined by:\\

\textit{Belief}(\textit{A}) = $\sum\limits_{B \subseteq A}m(B)\qquad$ for all $A
\subseteq \Theta$\\

\subsubsection{Plausibility Function}

Plausibility in a set, say \textit{A} of a frame $\Theta$ consisting of a
mutually exclusive and exhaustive set of elements, represents the maximum
possibility that a set \textit{A} is true given all the evidences. A
plausibility function \textit{Plausible} defined on a space $\Theta$ must satisfy the
following three properties:

\begin{itemize}
	\item[]\textit{Plausible}($\emptyset$) = 0
	\item[]\textit{Plausible}($\Theta$) = 1
	\item[]\small\textit{Plausible}($A_1 \cap \ldots A_n$)
	$\leq \sum\limits_i\textit{Plausible}$($A_i$) - $\sum\limits_{i \textless
	j}\textit{Plausible}$($A_i \cup A_j$) + \ldots +\\
	$(-1)^{n+1}\textit{Plausible}$($A_i \cup \ldots \cup A_n$)
\end{itemize}

A \textit{plausibility} measure is a function
$Plausible:2^\Theta\rightarrow[0,1]$, and is defined by:\\

\textit{Plausible}(\textit{A}) = $\sum\limits_{B \cap A \neq
\emptyset}m(B)\qquad$ for all $A \subseteq \Theta$\\

\noindent \textit{Plausible}(\textit{A}) in a subset \textit{A} is defined to
be the sum of all mass functions for the subsets \textit{B} that have non-zero
intersections with \textit{A}, and it represents the extent to which we fail to
disbelieve \textit{A}. In other words, it corresponds to the total belief that
does not contradict \textit{A}. The plausibility and belief functions are
related to one another, and we can represent this relation as:\\

\noindent\textit{Belief}(\textit{A}) = 1 -
\textit{Plausible}(\textit{$\neg$A}) $\quad$ and $\quad$
\textit{Plausible}(\textit{A}) = 1 - \textit{Belief}(\textit{$\neg$A}),\\

\noindent where \textit{$\neg$A} is \textit{A}'s complement. Also,
\textit{Belief}(\textit{$\neg$A}) is often called the \textit{doubt} in
\textit{A}. It is noteworthy to mention that Dempster-Shafer theory allows the
representation of \textit{ignorance} since \textit{Belief}(\textit{A}) = 0 does
not imply \textit{Belief}(\textit{$\neg$A}) $\textgreater$ 0 even though
\textit{Belief}(\textit{$\neg$A}) = 1 implies \textit{Belief}(\textit{A}) = 0. Other
notable relations are:\\

\noindent\textit{Belief}(\textit{A}) + \textit{Belief}(\textit{$\neg$A}) $\leq$ 1,
and\\

\noindent\textit{Plausible}(\textit{A}) +
\textit{Plausible}(\textit{$\neg$A}) $\geq$ 1.\\

Here, we also note that in the case of each of the focal elements being
singletons, we return back to traditional Bayesian analysis incorporating
normal probability theory, since in this case \textit{Belief}(\textit{A}) =
\textit{Plausible}(\textit{A}) \cite{beynon:dst-alternative-decision}.

\begin{figure}[tbh]
  \center
  \includegraphics[width=0.65\textwidth]{figure/uncertainty.png}
  \caption{Measures of belief and plausibility. The uncertainty interval is
  shaded gray. \cite{kay:dst-reliability}.}
  \label{fig:uncertainty}
\end{figure}

Collectively the above measures provide Dempster-Shafer theory with an explicit
measure of ignorance about \textit{A} and its complement. All the above measures
of confidence and the BPA are equivalent, in the sense that each of them can be
expressed as a function of any one of the rest. The \textit{uncertainty}
measure is defined as the length of the interval [\textit{Belief}(\textit{A}),
\textit{Plausible}(\textit{A})] where \textit{Belief}(\textit{A}) $\leqslant$
\textit{Plausible}(\textit{A}) \cite{yager:dst-combination-rules}, and it is
also called \textit{belief interval}. Figure \ref{fig:uncertainty} illustrates a
graphical representation of the belief, plausibility, and doubt measures which
we defined above. As it is shown and said earlier, the difference between
plausibility and belief describes the evidential interval range which represents
the uncertainty concerning the set \textit{A}. Also, as we see in Figure
\ref{fig:uncertainty}, lack of belief does not imply disbelief, since the
complements of belief and plausibility are doubt and disbelief, respectively.
Furthermore, the mass assigned to $\Theta$ can be interpreted as the global
ignorance, since the level of mass value is not discernible among the
hypotheses.

\subsubsection{Dempster's Rule of Combination}

Suppose that we have two pieces of uncertain evidence relevant to the same frame
of discernment $\Theta$. Dempster-Shafer theory also provides a method to
combine the measures of evidence from different sources, using Dempster's rule
of combination which combines two pieces of evidence into a single new piece.
The rule assumes that the sources are independent. If $m_1$ and $m_2$ are the
BPA's associated with $Bel_1$ and $Bel_2$ respectively and $Bel_1$ and $Bel_2$
are independent, then Dempster's rule of combination is as follows:\\

\[
	[m_1 \oplus m_2](y) = 
    \begin{dcases}
      0, & y = \emptyset \\
      \frac{\sum\limits_{A \cap B = y}m_1(A)m_2(B)}{1-\sum\limits_{A
      \cap B \neq \emptyset}m_1(A)m_2(B)}, & y
      \neq
      \emptyset
	\end{dcases}
\]\\

\noindent The numerator, i.e., $\sum\limits{_{A \cap B =y}}m_1(A)m_2(B)$,
represents the accumulated evidence for the sets A and B, which supports the
given hypothesis y. The denominator in the Dempster's rule of combination, i.e.,
$1-\sum\limits{_{A \cap B \neq \emptyset}}m_1(A)m_2(B)$, is an important
normalization factor denoted by $\mathcal{K}$ which can be interpreted as a
measure of conflict between the sources
\cite{srivastava:evidential-reasoning-uncertainty}.

\subsection{Fuzzy Logic Theory}
\label{sec:flt}

Fuzzy Logic, introduced by Zadeh in 1965 \cite{zadeh:fuzzy}, provides a
mathematical framework to capture uncertainty. Fuzziness manipulates uncertainty
by dealing with the boundaries of a set that are not clearly defined. Fuzzy
Logic is a multivalued logic, that allows intermediate values to be defined
between conventional evaluations like ``true'' and ``false''. Fuzzy Logic's
ultimate goal is to provide foundations for approximate reasoning using
imprecise propositions based on fuzzy set theory. In order to deal with such
imprecise inference, Fuzzy Logic allows the imprecise linguistic terms such as:
fuzzy predicates (e.g., old, expensive), fuzzy quantifiers (e.g., many, little),
and fuzzy truth values (e.g., unlikely false or unlikely true). Fuzzy Logic is a
method for reasoning with logical expressions describing membership in fuzzy
sets \cite{russell:ai-modern}. Logic as a base for reasoning can be essentially
distinguished by three items: truth values, operators, and reasoning procedures
(e.g., tautologies) \cite{zimmermann:fuzzy-sets}. For instance, in dual logic,
truth values can be ``true'' (1) or ``false'' (0), operators can be defined
using the truth tables, and modus ponens or contrapositions can be considered as
tautology. In Fuzzy Logic, the truth values are no longer restricted to two
values, but are expressed by the linguistic variables such as and including
``true" or ``false". In all forms of fuzzy reasoning, the implications can be
modeled in different ways.

\begin{figure}[tbh]
  \center
  \includegraphics[width=0.95\textwidth]{figure/fuzzy-algorithm.png}
  \caption{Fuzzy Logic algorithm.}
  \label{fig:fuzzy-algorithm}
\end{figure}

Figure \ref{fig:fuzzy-algorithm} shows the Fuzzy Logic algorithm. It begins
with initialization of linguistic variables (see Section
\ref{sec:linguistic-variables}) and construction of appropriate membership
functions (see Section \ref{sec:membership-function}) and the rule-base of the
fuzzy system (see Section \ref{sec:fuzzy-rules}). The constructed membership
functions transform the input data to fuzzy values (see Section
\ref{sec:fuzzification}). Then the inference system evaluates the constructed
rules with respect to the given input value, and merges the results obtained
from each rule. Finally, the overall result will be transformed to a non-fuzzy
(crisp) value (see Section \ref{sec:defuzzification}).

\subsubsection{Probability vs Possibility}

The theory of possibility is analogous and yet conceptually different from the
theory of probability. Based on the Fuzzy Logic theory of Zadeh
\cite{zadeh:fuzzy} there is a difference between the possibility of an event
happening and the probability of that. The following example by Zimmermann in
\cite{zimmermann:fuzzy-sets} shows the difference. Consider the statement ``Hans
ate \textit{X} eggs for breakfast'', where \textit{X} $\in$ \textit{U} = $\{1,
2, \ldots, 8\}$. We may associate a probability \textit{p} by observing
\textit{Hans} eating breakfast for 100 days,

\begin{figure}[tbh]
  \center
  \includegraphics[width=0.65\textwidth]{figure/prob-poss-01.png}
  \label{fig:probability}
\end{figure}

A fuzzy set expressing the degree to which \textit{Hans} can eat \textit{X} eggs
for breakfast may be the following possibility distribution $\pi$,

\begin{figure}[tbh]
  \center
  \includegraphics[width=0.65\textwidth]{figure/prob-poss-02.png}
  \label{fig:ppssibility}
\end{figure}

where the possibility of \textit{X} = 3 is 1, while the probability of
\textit{Hans} eating 3 eggs for breakfast is only 0.1. Therefore, as the example
shows, a possible event does not necessarily imply that it is probable too.
However, if the event is probable it must also be possible
\cite{zimmermann:fuzzy-sets}.

\subsubsection{Fuzzy Sets}

Fuzzy sets are a further development of the mathematical concept of conventional
or crisp sets. A fuzzy set is a class of objects with a continuum of degrees of
membership \cite{zadeh:fuzzy}. Following Zadeh \cite{zadeh:fuzzy} many sets have
more than an either-or criterion for membership, such as, the set young people,
which can contain people of different ages. A fuzzy set $\textit{A}$ is defined
by a membership function $\mu_A$ from the universe of discourse $\mathcal{X}$ to
the closed unit interval [0,1]. We interpret $\mu_A$($\textit{x}$) as the degree
of membership of $\textit{x}$ in \textit{A} (see Section
\ref{sec:membership-function}). Zadeh proposed this degree of membership, such
that the transition from membership to non-membership is gradual rather than
abrupt. Therefore, the degree of membership for all its members describes a
fuzzy set.

\subsubsection{Membership Functions}
\label{sec:membership-function}

\textit{Membership functions} are the crucial part of the Fuzzy Logic theory. In
fact, the difference between crisp (i.e., classical) and fuzzy sets is
established by introducing membership functions. Membership functions are
mathematical tools for indicating flexible membership to a set, modeling, and
quantifying the meaning of symbols. Membership functions are used in the
fuzzification and defuzzification steps (see Sections \ref{sec:fuzzification}
and \ref{sec:defuzzification}) of a Fuzzy Logic system. A membership function is
used to quantify a linguistic term (see Section \ref{sec:linguistic-variables}).
Therefore, the manipulation of fuzzy quantities can be accomplished by the
manipulation of fuzzy set membership functions. Some of the manipulation
includes set complement, intersection, and union as well as fuzzification and
defuzzification (see Sections \ref{sec:fuzzification} and
\ref{sec:defuzzification}) \cite{schalkoff:intelligent-systems}.

\begin{figure}[tbh]
  \center
  \includegraphics[width=0.65\textwidth]{figure/membership-function.png}
  \caption{Membership functions for the concepts young, mature and old.}
  \label{fig:membership-function}
\end{figure}

Let $\mathcal{X}$ be a crisp universal set. A fuzzy subset $\textit{A}$ of
$\mathcal{X}$ is characterized by a membership function; $\mu_A$ : $\mathcal{X}
\rightarrow$ [0, 1]. $\mu_A$($\textit{x}$) is called the \textit{membership
degree (grade)} of $\textit{x}$ in $\textit{A}$. The degree of membership is
expressed by a real number in the interval [0, 1]. The degree of membership is a
precise, but subjective measure that depends on the context.

Figure \ref{fig:membership-function} shows membership functions for three
linguistic terms of the variable age (see also Section
\ref{sec:linguistic-variables}). It shows three examples of membership functions
in the interval 0 to 70 years. These three functions define the degree of
membership of any given age in the sets of young, mature, and old ages. Note
that an important characteristic of fuzzy logic is that a numerical value does
not have to be fuzzified using only one membership function. In other words, a
value can belong to multiple sets at the same time. For instance, if someone is
20 years old her degree of membership in the set of young persons is 1.0
(maximum value), in the set of matures 0.35, and in the set of old persons 0.0
(minimum value). As another example, if someone is 50 years old the degrees of
membership in the sets of young, mature, and old are 0.0, 1.0, and 0.3
respectively.

Membership functions can have different shapes and their shapes can be
determined arbitrarily based on experience or sometimes by running statistical
studies on data. They can be sigmoidal, hyperbolic, Gaussian or any other
shape. The followings are some of the important properties of fuzzy sets and
membership functions:\\

\textbf{Height:} The height of a fuzzy set $\textit{A}$, denoted by
$\textit{h}$($\textit{A}$), corresponds to the upper bound of the membership
function. In other words, it is the largest membership degree obtained by any
element in that set:\\

$\textit{h}$($\textit{A}$) = sup$\{\mu_A(\textit{x}) | \textit{x} \in
\mathcal{X}\}$.\\

\textbf{Support:} The support of a fuzzy set $\textit{A}$ is a set of all
elements \textit{x} of $\mathcal{X}$ for which (\textit{x}, $\mu_A$(\textit{x}))
$\in$ $\textit{A}$ and $\mu_A$(\textit{x}) $\textgreater$ 0 holds. In other
words, support is a set of all elements of $\mathcal{X}$ that have non-zero
membership degrees in $\textit{A}$.\\

\textbf{$\alpha$-cut:} An $\alpha$-cut of a fuzzy set $\textit{A}$ is the subset
of elements with a membership degree greater than or equal to $\alpha$. The
$\alpha$-cut is denoted by:\\ \\$\alpha$-cut($\textit{A}$) = $\{\textit{x} \in
\mathcal{X} | \mu_A(\textit{x}) \geqslant \alpha\}$.\\

\textbf{core:} The core of a fuzzy set $\textit{A}$ is the crisp set that
contains all the elements of $\mathcal{X}$ that have the membership degrees of
\textbf{one} in $\textit{A}$.

\subsubsection{Linguistic Variables}
\label{sec:linguistic-variables}

The concept of membership functions discussed in Section
\ref{sec:membership-function} allows us to define fuzzy systems in natural
language. Linguistic variables are the input or output variables of the system
whose values are words or sentences from a natural language, instead of
numerical values. In fact, just like an algebraic variable takes numbers as
values, a linguistic variable takes words or sentences as values
\cite{zimmermann:fuzzy-sets}. A linguistic variable is generally decomposed into
a set of linguistic terms. For instance, for people's age, we usually use terms
such as ``old'' or ``young'' which are called linguistic values of the age.
Then, we can consider a set of decompositions for the linguistic variable age,
\textit{Age}(\textit{a}) = \{very-old, old, mature, young, very-young\}. The
members of this decomposition set are called \textit{linguistic terms} which
each cover a portion of the overall values of people's age. In other words, the
values that a linguistic variable can take is called its linguistic terms.

\subsubsection{Fuzzy Operators}

Fuzzy operators are used in order to manipulate fuzzy sets, and to evaluate the
constructed fuzzy rules (see Section \ref{sec:fuzzy-rules}), and ultimately to
combine the results of the individual rules. The operations on fuzzy sets are
different than the operations on classical sets. The definitions of operators on
fuzzy sets are not the same and can be arbitrarily chosen. Zadeh in
\cite{zadeh:fuzzy} defined the intersection (logical and), union (exclusive or),
and complement (negation) operations for fuzzy sets as generalizations of crisp
sets and of crisp statements. The operators for the complement (NOT), the
intersection (AND) and union (OR) that are most commonly used are:\\

\noindent The membership function of the \textbf{Intersection} of two fuzzy
sets \textit{A} and \textit{B}:\\

$\mu_{A \cap B}(X)$ = Min($\mu_{A}(X), \mu_{B}(X)$) $\qquad \forall x \in X$\\

\noindent The membership function of the \textbf{union} of two fuzzy sets
\textit{A} and \textit{B}:\\

$\mu_{A \cup B}(X)$ = Max($\mu_{A}(X), \mu_{B}(X)$) $\qquad \forall x \in X$\\

\noindent The membership function of the \textbf{complement} of a fuzzy set
\textit{A}:\\

$\mu_{A}(X)$ = 1 - $\mu_{A}(X) \qquad \forall x \in X$\\

\noindent These definitions were later extended by other reasearchers, e.g.,
\cite{yager:generalizing-leximin}.

\subsubsection{Fuzzy Rules}
\label{sec:fuzzy-rules}

In a Fuzzy Logic system, a rule-base is constructed to determine and control the
output variable. Fuzzy rules are simply comprised of IF-THEN rules which include
two parts of condition and conclusion. A fuzzy rule is encoded in a statement in
the following form:\\

\noindent \textbf{IF} (a statement of conditions is satisfied)

$\qquad\qquad\qquad\qquad\qquad$\textbf{THEN} (a set of consequences can
be inferred).\\

The followings are two examples of fuzzy
rules based on the age example depicted in Figure
\ref{fig:membership-function}:\\

\textbf{IF} (age is \textit{young}) \textbf{THEN} (run \textit{command
``talk''})
 
\textbf{IF} (age is \textit{old} \textbf{OR} \textit{mature}) \textbf{THEN} (run
\textit{command ``listen''})\\

\noindent The rules use the input membership values as weighting factors to
determine their influence on the fuzzy output sets of the final output
conclusion.

\subsubsection{Fuzzification}
\label{sec:fuzzification}

In order to map the crisp values to fuzzy ones, we need to evaluate their
membership degree using membership functions (see Section
\ref{sec:membership-function}). This process is called fuzzification and it
helps us to get one fuzzy value for each crisp input. Therefore, the
fuzzification process is mainly used to transform a crisp set to a fuzzy set.

\subsubsection{Reasoning in Fuzzy Logic}
\label{sec:reasoning}

In order to draw conclusions from a rule-base, we need a mechanism that can
produce an output from a collection of IF-THEN rules. Meaning, after evaluating
the result of each rule with respect to the given input value(s), the results of
the rules should be combined to obtain a final result. This process in Fuzzy
Logic systems is called reasoning. Fuzzy reasoning includes two distinct parts:
evaluating the IF part of the rule and applying the result to the consequent
(the THEN part of the rule). In fuzzy systems the evaluation is slightly
different than the classical rule-based systems. In fuzzy systems the IF part of
the rule is a fuzzy statement which means all the rules fire to some extent. If
the IF part of the rule is true in some degree of membership, then the
consequent is also true in some degree. It is noteworthy to mention that the
results of individual rules can be combined in different ways. There are
different types of accumulation methods that can be used to combine the results
of individual rules.

\subsubsection{Defuzzification}
\label{sec:defuzzification}

After the reasoning step, the Fuzzy Logic system provides the overall result as
a fuzzy value. Then, to obtain a final crisp output value, this fuzzy result
should be defuzzified which is the purpose of the defuzzifier component of a
Fuzzy Logic system. Defuzzification is performed according to the membership
function of the output variable. Figure \ref{fig:defuzzification} shows the
defuzzification step in a Fuzzy Logic system.

\begin{figure}[tbh]
  \center
  \includegraphics[width=0.55\textwidth]{figure/cog.png}
  \caption{Defuzzification using Center of Gravity (COG) method
  \cite{antonio:cog-defuzzification}.}
  \label{fig:defuzzification}
\end{figure}

There are different algorithms for the defuzzification step. One of the most
widely used algorithms is called \textit{centroid}, or \textit{Center of
Area}, or \textit{Center of Gravity} (COG). This method computes the center of
area of the region under the curve defined by a fuzzy set. In this method, the
defuzzified values tend to move smoothly in reaction to small changes, and it is
relatively easy to compute the value. In the following formula, \textit{A} is
a fuzzy set and $z_{_{COG}}$ is the final single crisp output which in this case
is obtained by COG method:\\

\begin{center}
$z_{_{COG}} = \frac{\sum\limits_{i=1}^{n} \mu_A(z_i).z_j}{\sum\limits_{i=1}^{n}
\mu_A(z_i)}$
\end{center}

In summary, Figure \ref{fig:fuzzy-system} (see also the fuzzy algorithm in
Figure \ref{fig:fuzzy-algorithm}) shows the process of fuzzy logic. Firstly, a
crisp set of input data are gathered and converted to a fuzzy set using fuzzy
linguistic variables (see Section \ref{sec:linguistic-variables}), fuzzy
linguistic terms and membership functions (see Section
\ref{sec:membership-function}). This step is known as fuzzification (see Section
\ref{sec:fuzzification}). Afterwards, an inference is made based on a set of
rules (see Section \ref{sec:reasoning}). Lastly, the resulting fuzzy output is
mapped to a crisp output using the membership functions, in the defuzzification
step (see Section \ref{sec:defuzzification}).

\begin{figure}[tbh]
  \center
  \includegraphics[width=0.66\textwidth]{figure/fuzzy-system.png}
  \caption{A Fuzzy Logic system.}
  \label{fig:fuzzy-system}
\end{figure}

\section{Advantages and Disadvantages of Theories of Uncertainty}
\label{sec:pros-cons}

Each of the three theories of uncertainty presented herein has certain
advantages and disadvantages. They are discussed in the following sections.

\subsection{Advantages and Disadvantages of Belief Networks}

Like any other computational formalism, Bayesian networks offer certain
advantages and disadvantages. Advantages of Bayesian networks include:

\begin{itemize}
  \item Transparent representation of causal relationships between variables
  which can facilitate understanding the causal relationships.
  
  \item Relatively easy recognition of dependencies and independencies between
  various nodes.
  
  \item The ability to handle situations where the data set is incomplete since
  the model accounts for dependencies between all variables.
  
  \item Capable of being readily updated when a new knowledge (evidence) becomes
  available.
  
  \item Both predictive/deductive and diagnostic/abductive reasonings are
  possible.

  \item Computational tractability exists for most practical applications.
\end{itemize}

\noindent The followings are the disadvantage of Bayesian networks:

\begin{itemize}
  \item A high level of effort is required to build network models where a
  significant amount of probability data is required due to an increasing number
  of nodes and links in the structure (possible large CPT sizes).
  
  \item Computationally intensive if the conditional independencies are not
  properly considered among the variables.
  
  \item Challenging to obtain experts' knowledge in the form of probability to
  build the network.
  
  \item No feedback loops in the Bayesian network's structure, which has an
  acyclic nature. This structure prevents typical feedback loops in design of
  Bayesian network models.
\end{itemize}

\subsection{Advantages and Disadvantages of Dempster-Shafer Theory}

Advantages of Dempster-Shafer theory are:

\begin{itemize}
  \item Addressing the concept of possibility.
  
  \item The ability to represent the concept of ignorance to allow one to
  specify a degree of ignorance in a situation, instead of being forced to
  supply prior probabilities.
  
  \item Consistent with classical probability theory.
  
  \item Distinguishing randomness from missing information.
  
  \item No required a priori knowledge.
  
  \item Including an evidence combination rule which provides an operator to
  integrate multiple pieces of information from different sources.
\end{itemize}

\noindent Disadvantages of Dempster-Shafer theory are:

\begin{itemize}
  \item Computational complexity grows exponentially with the number of
  hypotheses (in original formulation).
  
  \item Small modifications in the evidence assignments may lead to a
  completely different conclusion, which can lead to misleading and
  counter-intuitive results.
\end{itemize}

\subsection{Advantages and Disadvantages of Fuzzy Logic Theory}

Advantages of Fuzzy Logic theory are:

\begin{itemize}
  \item Describing algorithms in terms of a combination of numerics and
  linguistics.
  \item Capturing the concept of the ambiguity of information.
  \item Flexible and intuitive knowledge-base design.
  \item Easy computation.
  \item Relatively robust algorithms.
\end{itemize}

\noindent Disadvantages of Fuzzy Logic theory are:

\begin{itemize}
  \item Determining the exact fuzzy rules and membership functions is a hard
  task.
  \item Requires manual tuning to obtain a better result.
  \item Requires tuning in many options in design of a system.
  \item The order of inference steps matters.
  \item After reasoning, it can be difficult to exactly interpret the membership
  value.
  \item Validation of a fuzzy knowledge-base is typically expensive.
\end{itemize}

\section{Applications of Bayesian Networks}
\label{sec:applications}

There are different applications of Bayesian networks in robots and autonomous
agents such as motion control, sensory data fusion, and modeling domain
knowledge. In this section, we provide some of these applications.

In \cite{toussaint:bayesian-motion-planning} Toussaint proposes a new approach
to robotic motion control and planning based on probabilistic inference. His
method uses structured probabilistic models to represent a scenario and
efficient inference techniques (belief propagation) to solve planning problems.
He also uses Bayesian networks to find solutions to problems combining multiple
criteria or sources of information (sensor fusion) beside using them to solve a
fusion problem on the motor level. \cite{park:bn-service-robot} uses
probabilistic modeling for service robots to provide users with high-level
context-aware services required in home environments, and proposes a systematic
modeling approach for modeling a number of Bayesian networks. In
\cite{orozco:multisensor-bn} researchers have developed a system to integrate
indirect measures of different sensors. This system is designed based on a
Bayesian network and can be applied to use any type of sensor which provides
measures of the robot's environment independent of sensor types, environment and
application of the robot. In \cite{bourgault:operator-multiple-robot} authors
use Bayesian networks to model interactions between humans and multiple
semi-autonomous robots. They have modeled discrete operator decisions as
probabilistic Bayesian network blocks with conditional dependencies on
individual system states. In \cite{prado:robot-emotion-bayesian} researchers
propose a novel Bayesian approach to determine the emotional state the robot
shall assume according to how the interlocutor is talking to it through a
vocalization channel. In \cite{song:bn-service-robot} authors adopt Bayesian
Networks and an ontology together for modeling domain knowledge and reasoning
objects in a probabilistic framework for a service robot. In this work, they use
objects as context information for predicting the target object being present,
since it can be occluded or small in indoor environments. In
\cite{dearden:forward-model-robot} researchers use a Bayesian network to
represent a forward model of a robot's actions to predict the consequences of
the robot's actions on its own motor system and the environment. In
\cite{montesano:affordances-bn} researchers propose a general object affordance
model based on Bayesian networks linking actions, object features and action
effects. The network is learned by the robot through interaction with the
surrounding objects. \cite{vladareanu:robot-naviation-bayesian} presents the
navigation mobile robot systems for movement in non-stationary and
non-structured environments, using a Bayesian approach for avoiding obstacles
and dynamical stability control for motion on rough terrain. As another example,
in \cite{chong:bbn-decision-planning} researchers explore the use of Dynamic
Bayesian networks as a decision process to aid a mobile robot in planning its
surveillance route. In \cite{iwahashi:belief-system-hri} Iwahashi describes a
method that enables the robot to learn a system of beliefs through multi-modal
language interaction with the user. He uses the Bayesian learning method to
learn the robot's decision and confidence functions.
\cite{grimes:imitation-robot-bayesian} presents learning imitative whole body
motions in a humanoid robot using probabilistic inference in Bayesian networks.
Authors provide a model for exploiting prior information about whole-body
motions gathered from observing a human performance of the motion. 

There are other applications such as intention recognition for intelligent
human-robot interaction \cite{tahboub:hri-dbn}, mobile-robot localization
\cite{zhou:mobile-robot-localization} \cite{fox:bayesian-location}, and
autonomous reconstruction of 3D images for indoor mobile robots
\cite{delage:dbn-image-reconstruction}. Here, we also provide applications of
Bayesian networks in agents. 

In \cite{arinbjarnar:bn-agent-drama} researchers propose an efficient and
scalable Bayesian network technique using relevance reasoning, that are
suitable for the needs of their autonomous agents in directed emergent drama and
allow for real-time decision making. They do this by extracting only a relevant
part of the network and only update the values in this reduced network. In
\cite{xiang:dai-bn} Xiang extends Multiply Sectioned Bayesian networks (MSBNs)
for single-agent systems into a framework for multi-agent distributed
interpretation systems. In this work, each cooperative agent is represented as a
Bayesian subnet that consumes its own computational resource, gathers its own
evidence, and can answer queries. In \cite{saha:bayes-negotiation} authors
present a decision architecture of the arguing agent. They propose a novel
Bayesian network based an argumentation and decision making framework that
allows agents to utilize models of the other agents. In
\cite{grootswagers:bbn-applications} authors provide a review of Bayesian
networks' applications in different domains including image interpretation and
modeling cognitive processes. For all of the examples they explain how the
Bayesian models are implemented, their practical use and their limitations.
There are also other applications for Bayesian networks for the agent, such as
building internal models in multi-agent settings \cite{nielsen:fusion-bn-mas},
knowledge representation of the agents \cite{bloemeke:agent-encapsulated-bn}
\cite{santos:cognitive-knowledge-representation}
\cite{martin:reasoning-action-bbn}, modeling theory of mind
\cite{baker:bayesian-tom} \cite{bello:cginitive-foundation-tom}, and modeling
trustworthiness of agents \cite{tan:trust-bbn}.\\ \\ \\

\section{Conclusion}
\label{sec:conclusion}

In this response, we started by generally understanding the importance of
reasoning under uncertainty in intelligent machines. There are some theories
explaining similar or different aspects of uncertainty. Each of these theories
relates to the same concept of uncertainty from slightly different angles. For
instance, Bayesian networks rely on probability theory and assign one value
to each existing variable and relate them based on their causal relationship.
Dempster-Shafer theory as an evidential reasoning theory discusses whether there
is any evidence for a particular belief rather than probability of its truth,
and it is designed to deal with the distinction between uncertainty and ignorance.
And finally, the Fuzzy Logic theory has the power to deal with ambiguous
linguistic words and still provide reasonable results that gain enough credit
to be used as an underlying concept in many industrial controllers. However, all
of these theories have their own drawbacks in dealing with the key issue,
uncertainty (see Section \ref{sec:pros-cons}). As a result, it is one's
experience and the nature of the application which helps one to choose from
among these approaches to gain more accurate and reasonable results for the
application.

In our proposed work, we believe sources of uncertainty are everywhere in a
collaboration. The collaboration theories in general, and specifically
SharedPlans theory, are concerned about teamwork and the involvement of others
to form an intention, to generate or evolve the shared plan, or even to establish a
single mutual belief. To do so, one should rely on perceiving others' verbal and
nonverbal behaviors; in both cases there is a certain amount of uncertainty,
ambiguity and lack of evidence. Therefore, processes involved in this domain
need to be designed to address the existence of uncertainty if they are to 
autonomously generate and evolve the underlying structure of a collaboration
procedure. Moreover, beliefs, as the foundation of the underlying emotion-driven
processes, are involved in collaboration mechanisms, e.g. appraisal,
motivation.
Each individual belief includes a certain amount of uncertainty with respect to
whichever process has formed the belief, and under what situation. For instance,
sometimes the lack of evidence about a counterpart's belief about an event, or
the feeling of a counterpart for a collaborative action can cause irrelevant
responses based on false beliefs which could be at least mitigated by having a
mechanism to deal with uncertainty in some level. As we mentioned, the existence
of uncertainty appears in different constituents of a collaboration procedure
and it is for us to choose where to apply the appropriate mechanism to make more
stable collaborative behaviors.

\bibliographystyle{plain}
\bibliography{mshayganfar}

\end{document}




























